{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install faiss-cpu\n",
    "!pip install langchain_google_genai\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import PyPDF2\n",
    "import requests\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Text from PDF\n",
    "def extract_pdf_text(pdf_path):\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Chunk the Extracted Text\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = \" \".join(words[i:i + chunk_size])\n",
    "        if chunk:  # Ensure we don't add empty chunks\n",
    "            chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define Improved Retrieval Function with TF-IDF and FAISS\n",
    "class FAISSRetriever:\n",
    "    def __init__(self, chunks):\n",
    "        self.chunks = chunks\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "        if not chunks:\n",
    "            self.index = None\n",
    "            self.vectors = None\n",
    "            return\n",
    "\n",
    "        # Create TF-IDF matrix\n",
    "        self.vectors = self.vectorizer.fit_transform(chunks).toarray().astype('float32')\n",
    "\n",
    "        # Initialize FAISS index - using L2 distance\n",
    "        self.dimension = self.vectors.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "\n",
    "        # Add vectors to the index\n",
    "        faiss.normalize_L2(self.vectors)  # Normalize for cosine similarity\n",
    "        self.index.add(self.vectors)\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        if not self.chunks or self.index is None:\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # Vectorize the query\n",
    "            query_vector = self.vectorizer.transform([query]).toarray().astype('float32')\n",
    "            faiss.normalize_L2(query_vector)  # Normalize for cosine similarity\n",
    "\n",
    "            # Search using FAISS\n",
    "            distances, indices = self.index.search(query_vector, min(top_k, len(self.chunks)))\n",
    "\n",
    "            # Return the top-k chunks\n",
    "            return [self.chunks[i] for i in indices[0]]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in FAISS retrieval: {str(e)}\")\n",
    "            # Fallback to TF-IDF if FAISS fails\n",
    "            return self.tfidf_fallback(query, top_k)\n",
    "\n",
    "    def tfidf_fallback(self, query, top_k=3):\n",
    "        try:\n",
    "            query_vector = self.vectorizer.transform([query])\n",
    "            similarities = cosine_similarity(query_vector, self.vectors)[0]\n",
    "            top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "            return [self.chunks[i] for i in top_indices]\n",
    "        except Exception as e:\n",
    "            print(f\"Error in TF-IDF fallback: {str(e)}\")\n",
    "            # Final fallback to random selection\n",
    "            return random.sample(self.chunks, min(top_k, len(self.chunks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define Bloom's Taxonomy Levels with improved descriptions\n",
    "bloom_levels = {\n",
    "    \"Knowledge\": \"Create a question that tests recall of factual information, definitions, or key concepts from the text.\",\n",
    "    \"Comprehension\": \"Create a question that requires summarizing or explaining the main ideas in one's own words.\",\n",
    "    \"Application\": \"Create a question that asks how to apply concepts from the text to new situations or problems.\",\n",
    "    \"Analysis\": \"Create a question that requires analyzing the relationships between different components or examining the structure of ideas.\",\n",
    "    \"Synthesis\": \"Create a question that asks to combine ideas to create something new or propose alternative solutions.\",\n",
    "    \"Evaluation\": \"Create a question that requires making judgments based on criteria, critically assessing arguments, or evaluating the validity of ideas.\"\n",
    "}\n",
    " \n",
    "# Define answer word count ranges based on marks\n",
    "answer_word_counts = {\n",
    "    1: {\"min\": 20, \"max\": 50, \"description\": \"brief and concise\"},\n",
    "    3: {\"min\": 150, \"max\": 200, \"description\": \"moderately detailed\"},\n",
    "    5: {\"min\": 300, \"max\": 500, \"description\": \"comprehensive and in-depth\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Implement RAG with LangChain Gemini Integration\n",
    "def generate_question_with_gemini(retriever, mark, bloom_level, query, retry_count=2):\n",
    "    relevant_chunks = retriever.retrieve(query, top_k=3)\n",
    "\n",
    "    if not relevant_chunks:\n",
    "        return {\"error\": \"No relevant context found in the document\"}\n",
    "\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "\n",
    "    # Get the word count requirements for this mark value\n",
    "    word_count = answer_word_counts[mark]\n",
    "\n",
    "    # More detailed prompt with marking scheme and word count requirements\n",
    "    prompt = (\n",
    "        f\"Context from the document:\\n\\n{context}\\n\\n\"\n",
    "        f\"Based on the above context, create a {mark}-mark exam question at the '{bloom_level}' level of Bloom's Taxonomy.\\n\"\n",
    "        f\"For a {mark}-mark question, the student's answer should have {mark} distinct points or aspects to receive full marks.\\n\\n\"\n",
    "        f\"IMPORTANT: The model answer MUST be {word_count['min']}-{word_count['max']} words ({word_count['description']}).\\n\\n\"\n",
    "        f\"Format your response exactly as follows:\\n\"\n",
    "        f\"Question: [Write your question here]\\n\"\n",
    "        f\"Answer: [Provide a model answer that would receive full marks and is {word_count['min']}-{word_count['max']} words]\\n\"\n",
    "        f\"Marking Scheme: [List {mark} specific points that should be included for full marks]\\n\"\n",
    "        f\"Word Count: [Provide the exact word count of your answer]\"\n",
    "    )\n",
    "\n",
    "    # Initialize LangChain's Gemini model\n",
    "    try:\n",
    "        api_key = os.environ.get(\"GOOGLE_API_KEY\", \"AIzaSyDSQLLc2Pe25j-9FTPXuZYhptlmutkXSCk\")\n",
    "\n",
    "        # Initialize the LangChain ChatGoogleGenerativeAI object\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            google_api_key=api_key,\n",
    "            temperature=0.4,\n",
    "            max_output_tokens=1500,\n",
    "            max_retries=retry_count,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        # Define system message for role context\n",
    "        system_message = \"You are an expert education professional creating exam questions based on provided text. Your task is to generate questions with answers that strictly adhere to specified word count requirements.\"\n",
    "\n",
    "        for attempt in range(retry_count + 1):\n",
    "            try:\n",
    "                print(f\"Attempt {attempt+1}: Making request with LangChain to Gemini\")\n",
    "\n",
    "                # Construct the messages\n",
    "                from langchain_core.messages import SystemMessage, HumanMessage\n",
    "                messages = [\n",
    "                    SystemMessage(content=system_message),\n",
    "                    HumanMessage(content=prompt)\n",
    "                ]\n",
    "\n",
    "                # Invoke the model\n",
    "                response = llm.invoke(messages)\n",
    "                generated_text = response.content\n",
    "\n",
    "                # Extract question, answer, marking scheme, and word count\n",
    "                question_part = \"\"\n",
    "                answer_part = \"\"\n",
    "                marking_scheme = \"\"\n",
    "                word_count_actual = 0\n",
    "\n",
    "                # Parse the response robustly\n",
    "                if \"Question:\" in generated_text:\n",
    "                    parts = generated_text.split(\"Question:\", 1)\n",
    "                    remaining = parts[1]\n",
    "\n",
    "                    if \"Answer:\" in remaining:\n",
    "                        question_answer = remaining.split(\"Answer:\", 1)\n",
    "                        question_part = question_answer[0].strip()\n",
    "\n",
    "                        if \"Marking Scheme:\" in question_answer[1]:\n",
    "                            answer_marking = question_answer[1].split(\"Marking Scheme:\", 1)\n",
    "                            answer_part = answer_marking[0].strip()\n",
    "\n",
    "                            if \"Word Count:\" in answer_marking[1]:\n",
    "                                marking_wordcount = answer_marking[1].split(\"Word Count:\", 1)\n",
    "                                marking_scheme = marking_wordcount[0].strip()\n",
    "\n",
    "                                # Try to extract the actual word count\n",
    "                                word_count_text = marking_wordcount[1].strip()\n",
    "                                try:\n",
    "                                    word_count_actual = int(''.join(filter(str.isdigit, word_count_text.split()[0])))\n",
    "                                except:\n",
    "                                    # Count words in answer if extraction fails\n",
    "                                    word_count_actual = len(answer_part.split())\n",
    "                            else:\n",
    "                                marking_scheme = answer_marking[1].strip()\n",
    "                                word_count_actual = len(answer_part.split())\n",
    "                        else:\n",
    "                            answer_part = question_answer[1].strip()\n",
    "                            word_count_actual = len(answer_part.split())\n",
    "\n",
    "                # Verify word count is within requirements\n",
    "                actual_word_count = len(answer_part.split())\n",
    "                within_limits = word_count[\"min\"] <= actual_word_count <= word_count[\"max\"]\n",
    "\n",
    "                # If not within limits and we have retries left, try again\n",
    "                if not within_limits and attempt < retry_count:\n",
    "                    print(f\"Answer word count ({actual_word_count}) outside required range ({word_count['min']}-{word_count['max']}). Retrying...\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "\n",
    "                return {\n",
    "                    \"question\": question_part,\n",
    "                    \"answer\": answer_part,\n",
    "                    \"marking_scheme\": marking_scheme,\n",
    "                    \"bloom_taxonomy\": bloom_level,\n",
    "                    \"marks\": mark,\n",
    "                    \"word_count\": actual_word_count,\n",
    "                    \"retrieved_context\": context,  # Include the retrieved context\n",
    "                    \"retrieved_chunks\": relevant_chunks  # Also include individual chunks for more detailed analysis\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in LangChain Gemini request (attempt {attempt+1}/{retry_count+1}): {str(e)}\")\n",
    "                if attempt < retry_count:\n",
    "                    wait_time = min(2 * (attempt + 1), 10)  # Exponential backoff\n",
    "                    print(f\"Waiting {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                return {\"error\": f\"LangChain request failed: {str(e)}\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize LangChain model: {str(e)}\")\n",
    "        return {\"error\": f\"Failed to initialize LangChain model: {str(e)}\"}\n",
    "\n",
    "    return {\"error\": \"Failed to generate question after multiple attempts\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # At the beginning of your main block, add this function to validate the API key\n",
    "# def test_api_connection(api_key):\n",
    "#     \"\"\"Test the API connection with a simple request using LangChain\"\"\"\n",
    "#     try:\n",
    "#         print(\"Testing API connection...\")\n",
    "\n",
    "#         # Initialize LangChain model with minimal settings for a quick test\n",
    "#         llm = ChatGoogleGenerativeAI(\n",
    "#             model=\"gemini-2.0-flash\",\n",
    "#             google_api_key=api_key,\n",
    "#             temperature=0.1,\n",
    "#             max_output_tokens=50,\n",
    "#             timeout=10\n",
    "#         )\n",
    "\n",
    "#         # Simple test message\n",
    "#         from langchain_core.messages import HumanMessage\n",
    "#         test_message = HumanMessage(content=\"Hello, please respond with 'API connection successful' if you receive this message.\")\n",
    "\n",
    "#         # Make a simple request\n",
    "#         response = llm.invoke([test_message])\n",
    "\n",
    "#         if response and response.content:\n",
    "#             print(\"API connection test successful!\")\n",
    "#             return True\n",
    "#         else:\n",
    "#             print(\"API test failed: No valid response content\")\n",
    "#             return False\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"API test failed with exception: {str(e)}\")\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format output for each question - now including retrieved context\n",
    "def format_question_output(question_data):\n",
    "    \"\"\"Format question data according to the required output format with retrieved context\"\"\"\n",
    "    return {\n",
    "        \"Question\": question_data[\"question\"],\n",
    "        \"Answer\": question_data[\"answer\"],\n",
    "        \"Bloom's Taxonomy\": question_data[\"bloom_taxonomy\"],\n",
    "        \"Marks\": question_data[\"marks\"],\n",
    "        \"Retrieved Context\": question_data[\"retrieved_context\"],  # Include full concatenated context\n",
    "        \"Individual Context Chunks\": question_data.get(\"retrieved_chunks\", [])  # Include individual chunks\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from hess205.pdf...\n",
      "Chunking text...\n",
      "Created 20 chunks\n",
      "Initializing FAISS retriever...\n",
      "Generating questions...\n",
      "Generating a 1-mark question at Knowledge level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 1-mark question with 34 words: Who was proclaimed the leader of the sepoys by the...\n",
      "Retrieved context length: 5158 characters\n",
      "Generating a 1-mark question at Comprehension level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 1-mark question with 30 words: What action by the British ignited the revolt in M...\n",
      "Retrieved context length: 5344 characters\n",
      "Generating a 1-mark question at Application level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 1-mark question with 31 words: Based on the text, how could the British have pote...\n",
      "Retrieved context length: 5152 characters\n",
      "Generating a 1-mark question at Analysis level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 1-mark question with 38 words: Analyze how the annexation of Awadh contributed to...\n",
      "Retrieved context length: 5259 characters\n",
      "Generating a 1-mark question at Synthesis level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 1-mark question with 33 words: How did the sepoys' actions in Meerut and Delhi tr...\n",
      "Retrieved context length: 5242 characters\n",
      "Generating a 3-mark question at Knowledge level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Answer word count (133) outside required range (150-200). Retrying...\n",
      "Attempt 2: Making request with LangChain to Gemini\n",
      "Answer word count (142) outside required range (150-200). Retrying...\n",
      "Attempt 3: Making request with LangChain to Gemini\n",
      "Answer word count (149) outside required range (150-200). Retrying...\n",
      "Attempt 4: Making request with LangChain to Gemini\n",
      "Generated 3-mark question with 179 words: What actions did the British take to appease the p...\n",
      "Retrieved context length: 5158 characters\n",
      "Generating a 3-mark question at Comprehension level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 3-mark question with 173 words: Describe the events that unfolded in Meerut and De...\n",
      "Retrieved context length: 5344 characters\n",
      "Generating a 3-mark question at Application level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 3-mark question with 171 words: How did the actions of the British administration,...\n",
      "Retrieved context length: 5152 characters\n",
      "Generating a 3-mark question at Analysis level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 3-mark question with 196 words: Analyze the factors that contributed to the transf...\n",
      "Retrieved context length: 5259 characters\n",
      "Generating a 3-mark question at Synthesis level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 3-mark question with 160 words: Synthesize information from the text to explain ho...\n",
      "Retrieved context length: 5242 characters\n",
      "Generating a 5-mark question at Knowledge level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 5-mark question with 373 words: Describe the actions taken by the British governme...\n",
      "Retrieved context length: 5158 characters\n",
      "Generating a 5-mark question at Comprehension level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 5-mark question with 396 words: Describe the key factors that led to the revolt by...\n",
      "Retrieved context length: 5344 characters\n",
      "Generating a 5-mark question at Application level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in LangChain Gemini request (attempt 1/4): 429 Resource has been exhausted (e.g. check quota).\n",
      "Waiting 2 seconds before retrying...\n",
      "Attempt 2: Making request with LangChain to Gemini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer word count (520) outside required range (300-500). Retrying...\n",
      "Attempt 3: Making request with LangChain to Gemini\n",
      "Generated 5-mark question with 483 words: The events of 1857, triggered by the controversial...\n",
      "Retrieved context length: 5152 characters\n",
      "Generating a 5-mark question at Analysis level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Answer word count (529) outside required range (300-500). Retrying...\n",
      "Attempt 2: Making request with LangChain to Gemini\n",
      "Generated 5-mark question with 435 words: Analyze the multiple factors that contributed to t...\n",
      "Retrieved context length: 5259 characters\n",
      "Generating a 5-mark question at Synthesis level (attempt 1)...\n",
      "Attempt 1: Making request with LangChain to Gemini\n",
      "Generated 5-mark question with 494 words: Synthesize the various factors that contributed to...\n",
      "Retrieved context length: 5242 characters\n",
      "\n",
      "Example of formatted question output (truncated for readability):\n",
      "{\n",
      "  \"Question\": \"Who was proclaimed the leader of the sepoys by the soldiers in Delhi?\",\n",
      "  \"Answer\": \"Bahadur Shah Zafar, the Mughal emperor, was proclaimed as their leader by the sepoys in Delhi after they rebelled against the British. The soldiers forced their way into his palace to make this proclamation.\",\n",
      "  \"Bloom's Taxonomy\": \"Knowledge\",\n",
      "  \"Marks\": 1,\n",
      "  \"Retrieved Context\": \"British henceforth adopted a policy of \\u2018leniency, indulgence and forbearance\\u2019 towards the people of Khurda. The price of salt was reduced and necessar...\",\n",
      "  \"Individual Context Chunks\": [\n",
      "    \"British henceforth adopted a policy of \\u2018leniency, indulgence and forbearance\\u2019 towards the people of ...\",\n",
      "    \"had collapsed for good and gave them the confidence to take the plunge and join the uprising. A situ...\",\n",
      "    \"new cartridges, which were suspected of being coated with the fat of cows and pigs. Eighty-five sepo...\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Successfully generated 15 questions with the following distribution:\n",
      "  1-mark questions: 5\n",
      "  3-mark questions: 5\n",
      "  5-mark questions: 5\n",
      "Questions saved to generated_questions.json\n"
     ]
    }
   ],
   "source": [
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the API key\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDSQLLc2Pe25j-9FTPXuZYhptlmutkXSCk\"\n",
    "\n",
    "    # Test API connection before proceeding\n",
    "    # if not test_api_connection(os.environ[\"GOOGLE_API_KEY\"]):\n",
    "    #     print(\"API connection test failed. Please check your API key and try again.\")\n",
    "    #     exit(1)\n",
    "\n",
    "    # Define your PDF file path here - use an environment variable or config file in production\n",
    "    PDF_PATH = \"hess205.pdf\"\n",
    "\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        print(f\"Error: PDF file not found at {PDF_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    # Extract text from PDF\n",
    "    print(f\"Extracting text from {PDF_PATH}...\")\n",
    "    pdf_text = extract_pdf_text(PDF_PATH)\n",
    "\n",
    "    if not pdf_text:\n",
    "        print(\"Error: Could not extract text from PDF\")\n",
    "        exit(1)\n",
    "\n",
    "    # Create chunks\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(pdf_text, chunk_size=300, overlap=50)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "\n",
    "    # Initialize FAISS retriever\n",
    "    print(\"Initializing FAISS retriever...\")\n",
    "    retriever = FAISSRetriever(chunks)\n",
    "\n",
    "    # Generate specific number of questions for each mark value\n",
    "    questions = []\n",
    "    marks_distribution = {1: 5, 3: 5, 5: 5}  # 3 questions of each mark value\n",
    "    taxonomy_levels = list(bloom_levels.keys())\n",
    "\n",
    "    print(\"Generating questions...\")\n",
    "\n",
    "    # Generate questions according to the required distribution\n",
    "    for mark, count in marks_distribution.items():\n",
    "        generated_count = 0\n",
    "        retries = 0\n",
    "        max_retries = 10  # Max number of attempts per mark value\n",
    "\n",
    "        # Try to generate the required number of questions for this mark value\n",
    "        while generated_count < count and retries < max_retries:\n",
    "            # Distribute taxonomy levels evenly\n",
    "            level = taxonomy_levels[generated_count % len(taxonomy_levels)]\n",
    "            query = bloom_levels[level]\n",
    "\n",
    "            print(f\"Generating a {mark}-mark question at {level} level (attempt {retries+1})...\")\n",
    "            q = generate_question_with_gemini(retriever, mark, level, query, retry_count=3)\n",
    "\n",
    "            if \"error\" in q:\n",
    "                print(f\"Error: {q['error']}\")\n",
    "                retries += 1\n",
    "                time.sleep(1)  # Slight delay before retrying\n",
    "            else:\n",
    "                # Verify the word count is within the specified range\n",
    "                word_count = q.get(\"word_count\", 0)\n",
    "                word_count_range = answer_word_counts[mark]\n",
    "\n",
    "                if word_count_range[\"min\"] <= word_count <= word_count_range[\"max\"]:\n",
    "                    questions.append(q)\n",
    "                    print(f\"Generated {mark}-mark question with {word_count} words: {q['question'][:50]}...\")\n",
    "                    print(f\"Retrieved context length: {len(q['retrieved_context'])} characters\")\n",
    "                    generated_count += 1\n",
    "                else:\n",
    "                    print(f\"Rejected question with {word_count} words (outside range {word_count_range['min']}-{word_count_range['max']})\")\n",
    "                    retries += 1\n",
    "\n",
    "    # Sort questions by mark value\n",
    "    questions.sort(key=lambda q: q.get(\"marks\", 0))\n",
    "\n",
    "    # Format questions according to the required output format\n",
    "    formatted_questions = [format_question_output(q) for q in questions]\n",
    "\n",
    "    # Save questions to JSON file in the required format\n",
    "    output_file = \"generated_questions.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(formatted_questions, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Print example of formatted output\n",
    "    if formatted_questions:\n",
    "        print(\"\\nExample of formatted question output (truncated for readability):\")\n",
    "        example = formatted_questions[0].copy()\n",
    "\n",
    "        # Truncate context for display purposes only\n",
    "        if \"Retrieved Context\" in example:\n",
    "            context_preview = example[\"Retrieved Context\"][:150] + \"...\" if len(example[\"Retrieved Context\"]) > 150 else example[\"Retrieved Context\"]\n",
    "            example[\"Retrieved Context\"] = context_preview\n",
    "\n",
    "        if \"Individual Context Chunks\" in example and example[\"Individual Context Chunks\"]:\n",
    "            for i, chunk in enumerate(example[\"Individual Context Chunks\"]):\n",
    "                example[\"Individual Context Chunks\"][i] = chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
    "\n",
    "        print(json.dumps(example, indent=2))\n",
    "\n",
    "    # Print summary\n",
    "    mark_counts = {}\n",
    "    for q in questions:\n",
    "        mark = q.get(\"marks\", 0)\n",
    "        mark_counts[mark] = mark_counts.get(mark, 0) + 1\n",
    "\n",
    "    print(f\"\\nSuccessfully generated {len(questions)} questions with the following distribution:\")\n",
    "    for mark, count in mark_counts.items():\n",
    "        print(f\"  {mark}-mark questions: {count}\")\n",
    "    print(f\"Questions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
